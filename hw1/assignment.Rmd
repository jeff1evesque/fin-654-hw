---
title: "Project 1 -- HO2 Analysis"
subtitle: "Live Sessions: weeks 1 and 2"
output: word_document
toc: TRUE
toc_float: TRUE
---

## Part 1

In this set we will build a data set using filters and `if` and `diff` statements. We will then answer some questions using plots and a pivot table report. We will then write a function to house our approach in case we would like to run the same analysis on other data sets.

### Problem

Supply chain managers at our company continue to note we have a significant exposure to heating oil prices (Heating Oil No. 2, or HO2), specifically New York Harbor. The exposure hits the variable cost of producing several products. When HO2 is volatile, so is earnings. Our company has missed earnings forecasts for five straight quarters. To get a handle on HO2 we download this data set and review some basic aspects of the prices. 

```{r eval = TRUE}
# Read in data
# package EIAdata
#
HO2 = read.csv("../data/nyhh02.csv", header = T, stringsAsFactors = F)
# stringsAsFactors sets dates as character type
head(HO2)
HO2 = na.omit(HO2) ## to clean up any missing data
# use na.approx() as well
str(HO2) # review the structure of the data so far
```

### Questions

1. What is the nature of HO2 returns? We want to reflect the ups and downs of price movements, something of prime interest to management. First, we calculate percentage changes as log returns. Our interest is in the ups and downs. To look at that we use `if` and `else` statements to define a new column called `direction`. We will build a data frame to house this analysis.

```{r eval = TRUE}
# Construct expanded data frame
return1 = as.numeric(diff(log(HO2$DHOILNYH))) * 100 # Euler 
size1 = as.numeric(abs(return1)) # size is indicator of volatility
direction1 = ifelse(return1 > 0, "up", ifelse(return1 < 0, "down", "same")) # another indicator of volatility
date1 = as.Date(HO2$DATE[-1], "%m/%d/%Y") # length of DATE is length of return +1: omit 1st observation
price1 = as.numeric(HO2$DHOILNYH[-1]) # length of DHOILNYH is length of return +1: omit first observation
price1 = as.numeric(diff(log(HO2$DHOILNYH)))
HO2.df1 = na.omit(data.frame(
    date = date1,
    price = price1,
    return = return1,
    size = size1,
    direction = direction1
))

return = HO2$DHOILNYH
size = abs(return)
direction = ifelse(return > 0, "up", ifelse(return < 0, "down", "same"))
date = as.Date(HO2$DATE, "%m/%d/%Y")
price = as.numeric(HO2$DHOILNYH)
HO2.df = na.omit(data.frame(
    date = date,
    price = price,
    return = return,
    size = size,
    direction = direction
))
``` 

We can plot with the `ggplot2` package. In the `ggplot` statements we use `aes`, "aesthetics", to pick `x` (horizontal) and `y` (vertical) axes. Use `group =1` to ensure that all data is plotted. The added (`+`) `geom_line` is the geometrical method that builds the line plot.

```{r echo = TRUE, eval = TRUE}
library(ggplot2)
library(gridExtra)
p1 = ggplot(HO2.df, aes(x = date, y = return, group = 1)) +
    geom_line(colour = "blue") +
    ggtitle('Standard Plot: 1st element removed')

p2 = ggplot(HO2.df1, aes(x = date1, y = return1, group = 1)) +
    geom_line(colour = "blue") +
    ggtitle('Normalized Data')
grid.arrange(p1, p2, nrow=2)
```

Let's try a bar graph of the absolute value of price rates. We use `geom_bar` to build this picture.

```{r echo = TRUE, eval = TRUE}
library(ggplot2)
p3 = ggplot(HO2.df1, aes(x = date, y = size, group = 1)) +
    geom_bar(stat = "identity", colour = "green") +
    ggtitle('Absolute Value: Price Rates')

p4 = ggplot(HO2.df1, aes(date, size)) +
    geom_bar(stat = "identity", colour = "darkorange") +
    geom_line(data = HO2.df, aes(date, return), colour = "blue") +
    ggtitle('Return on Size Overlay')

grid.arrange(p3, p4, nrow=2)
```

2. Let's dig deeper and compute mean, standard deviation, etc. Load the `data_moments()` function. Run the function using the `HO2.df1$return` subset of the data and write a `knitr::kable()` report.

```{r echo = TRUE, eval = TRUE}
data_moments = function(data) {
    library(moments)
    mean.r = mean(data)
    sd.r = sd(data)
    median.r = median(data)
    skewness.r = skewness(data)
    kurtosis.r = kurtosis(data)
    result = data.frame(
        mean = mean.r,
        std_dev = sd.r,
        median = median.r,
        skewness = skewness.r,
        kurtosis = kurtosis.r
    )

    return(result)
}

# Run data_moments()
answer = data_moments(HO2.df1$return)

# Build pretty table
answer = round(answer, 4)
knitr::kable(answer)
```

3. Let's pivot `size` and `return` on `direction`. What is the average and range of returns by direction? How often might we view positive or negative movements in HO2?

```{r echo = TRUE, eval = TRUE}
# Counting
table(HO2.df1$return < 0) # one way
table(HO2.df1$return > 0)
table(HO2.df1$direction) # this counts 0 returns as negative
table(HO2.df1$return == 0)

# Pivoting
library(dplyr)
## 1: filter to those houses with fairly high prices
# pivot.table =filter(HO2.df, size > 0.5*max(size))
## 2: set up data frame for by-group processing
pivot.table =group_by(HO2.df1, direction)
## 3: calculate the summary metrics
options(dplyr.width = Inf) ## to display all columns
HO2.count = length(HO2.df1$return)
pivot.table =summarise(pivot.table, return.avg = round(mean(return), 4), return.sd = round(sd(return), 4), quantile.5 = round(quantile(return, 0.05), 4), quantile.95 = round(quantile(return, 0.95), 4), percent = round((length(return)/HO2.count)*100, 2))
# Build visual
knitr::kable(pivot.table, digits = 2)
# Here is how we can produce a LaTeX formatted and rendered table
# 
library(xtable)
HO2.caption = "Heating Oil No. 2: 1986-2016"
print(xtable(t(pivot.table), digits = 2, caption = HO2.caption, align=rep("r", 4), table.placement="V"))
print(xtable(answer), digits = 2)
```

## Part 2

We will use the data from Part 1 to investigate the distribution of returns we generated. This will entail fitting the data to some parametric distributions as well as 

### Problem

We want to further characterize the distribution of up and down movements visually. Also we would like to repeat the analysis periodically for inclusion in management reports.

### Questions

1. How can we show the differences in the shape of ups and downs in HO2, especially given our tolerance for risk? We can use the `HO2.df` data frame with `ggplot2` and the cumulative relative frequency function `stat_ecdf` to begin to understand this data.

```{r echo = TRUE, eval = TRUE}
HO2.tol.pct = 0.95
HO2.tol = quantile(HO2.df$return, HO2.tol.pct)
HO2.tol.label = paste("Tolerable Rate = ", round(HO2.tol, 2), sep = "")
ggplot(HO2.df, aes(return, fill = direction)) + stat_ecdf(colour = "blue", size = 0.75) + geom_vline(xintercept = HO2.tol, colour = "red", size = 1.5) + annotate("text", x = HO2.tol+5 , y = 0.75, label = HO2.tol.label, colour = "darkred")
```

2. How can we regularly, and reliably, analyze HO2 price movements? For this requirement, let's write a function similar to `data_moments`. Name this new function `HO2_movement()`.

```{r echo = TRUE, echo = TRUE}
## HO2_movement(file, caption)
## input: HO2 csv file from /data directory
## output: result for input to kable in $table and xtable in $xtable; 
## data frame for plotting and further analysis in $df.
## Example: HO2.data = HO2_movement(file = "../data/nyhh02.csv", caption = "HO2 NYH")
HO2_movement = function(file = "../data/nyhh02.csv", caption = "Heating Oil No. 2: 1986-2016") {
    # Read file and deposit into variable
    HO2 = read.csv(file, header = T, stringsAsFactors = F)

    # stringsAsFactors sets dates as character type
    HO2 = na.omit(HO2) ## to clean up any missing data

    # Construct expanded data frame
    return = as.numeric(diff(log(HO2$DHOILNYH))) * 100
    size = as.numeric(abs(return)) # size is indicator of volatility
    direction = ifelse(return > 0, "up", ifelse(return < 0, "down", "same")) # another indicator of volatility
    date = as.Date(HO2$DATE[-1], "%m/%d/%Y") # length of DATE is length of return +1: omit 1st observation
    price = as.numeric(HO2$DHOILNYH[-1]) # length of DHOILNYH is length of return +1: omit first observation
    HO2.df = na.omit(data.frame(
        date = date,
        price = price,
        return = return,
        size = size,
        direction = direction
    )) # clean up data frame by omitting NAs

    require(dplyr)
    ## 1: filter if necessary
    # pivot.table =filter(HO2.df, size > 0.5*max(size))

    ## 2: set up data frame for by-group processing
    pivot.table =group_by(HO2.df, direction)

    ## 3: calculate the summary metrics
    options(dplyr.width = Inf) ## to display all columns
    HO2.count = length(HO2.df$return)
    pivot.table =summarise(
        pivot.table,
        return.avg = mean(return),
        return.sd = sd(return),
        quantile.5 = quantile(return, 0.05),
        quantile.95 = quantile(return, 0.95),
        percent = (length(return)/HO2.count)*100
    )

    # Construct transpose of pivot table with xtable()
    require(xtable)
    pivot.xtable = xtable(
        t(pivot.table),
        digits = 2,
        caption = caption,
        align=rep("r", 4),
        table.placement="V"
    )
    HO2.caption = "Heating Oil No. 2: 1986-2016"
    output.list = list(
        table = pivot.table,
        xtable = pivot.xtable,
        df = HO2.df
    )

    return(output.list)
}
```

Test `HO2_movement()` with data and display results in a table with `2` decimal places.
```{r echo = TRUE, eval = TRUE}
knitr::kable(HO2_movement(file = "../data/nyhh02.csv")$table, digits = 2)
```

Morale: more work today (build the function) means less work tomorrow (write yet another report).

3. Suppose we wanted to simulate future movements in HO2 returns. What distribution might we use to run those scenarios? Here, let's use the `MASS` package's `fitdistr()` function to find the optimal fit of the HO2 data to a parametric distribution. We will use the `gamma` distribution to simulate future heating oil \#2 price scenarios.

```{r echo = TRUE, eval = TRUE}
library(MASS)
HO2.data = HO2_movement(file = "../data/nyhh02.csv", caption = "HO2 NYH")$df
str(HO2.data)
fit.gamma.up = fitdistr(
    HO2.data[HO2.data$direction == "up", "return"],
    "gamma",
    hessian = TRUE
)

# fit.t.same = fitdistr(HO2.data[HO2.data$direction == "same", "return"], "gamma", hessian = TRUE) # a problem here is all observations = 0

# define gamma density
x = rgamma(100, shape = fit.gamma.up$sd[1], scale = 1/fit.gamma.up$estimate[2])
den = density(x)
dat = data.frame(x = den$x, y = den$y)

# plot density as points
ggplot(data = dat, aes(x = x, y = y)) +
  geom_point(size = 3) +
  theme_classic()

# gamma distribution properties
fit.t.down = fitdistr(HO2.data[HO2.data$direction == "down", "return"], "t", hessian = TRUE)
fit.t.down
fit.gamma.down = fitdistr(-HO2.data[HO2.data$direction == "down", "return"], "gamma", hessian = TRUE)
fit.gamma.down
```


## Conclusion

### Skills and Tools
The main tools implemented for this report include RStudio, with associated built-in functions, as well as the following additional packages:

* `ggplot2`
* `gridExtra`
* `dplyr`
* `xtable`
* `MASS`
* `tseries`
* `stats`

Since timeseries data on "New York Harbor No. 2" (HO2) oil prices was used, the ability to interpret stationarity, as well as autocorrelation and partial autocorrelation was required. Standard plotting techniques using `ggplot2`, followed by the "dickey-fuller" test, via the `tseries` package, help determine whether data transformations were required. In this assignment, the `log` transformation followed by a difference of 1-step was implemented on the data. Having the data properly transformed, allowed the use of pivot tables from the `dplyr` package. Particular aspects were highlighted, and related to real world events. Additionally, a number of steps were taken to generate a gamma distribution. As later explained in the "Business Remarks" (below), the ability to translate properties of the gamma distribution, including `shape`, and `size` allowed an explanation regarding key properites for the given data distribution.

### Data Insights
A csv dataset containing prices for the "New York Harbor No. 2" (HO2) oil prices was analyzed for volatility, and associated earnings. The corresponding data was a two column dataset containing the `DATE`, as well as the `DHOILNYH` oil price. Simple computations were performed, in order to obtain the required dataframe structure:

```{r}
return = HO2$DHOILNYH
size = abs(return)
direction = ifelse(return > 0, "up", ifelse(return < 0, "down", "same"))
date = as.Date(HO2$DATE, "%m/%d/%Y")
price = as.numeric(HO2$DHOILNYH)

HO2.df = na.omit(data.frame(
    date = date,
    price = price,
    return = return,
    size = size,
    direction = direction
))
```

The first two columns of the `HO2.df` dataframe are self-evident, while the `return` is the original oil price, corresponding to the `DHOILNYH` column. The `size` was the absolute value of the oil price, which indicates the volatility of the price. A plot was easily generated:

```{r echo = TRUE, eval = TRUE}
library(ggplot2)
library(gridExtra)
ggplot(HO2.df, aes(x = date, y = return, group = 1)) +
    geom_line(colour = "blue") +
    ggtitle('Standard Plot: 1st element removed')
```

It is evident that the corresponding data doesn't exhibit any noticeable patterns. Specifically, the mean, variance, and autocorrelation is not constant over time. However, to prove non-stationarity, the dickey-fuller test was applied to the `HO2.df$price`:

```{r}
library(tseries)
adf.test(HO2.df$price)
```

Since the `p-value = 0.5126`, we fail to reject the null hypothesis, and conclude the given data is not stationary. To better prepare the data for analysis, some tranformation was required. Specifically, the `log` function was applied to the oil price, then the `diff` was aplied with the default `lag=1` and `difference=1`.


```{r}
return1 = as.numeric(diff(log(HO2$DHOILNYH)), 2)
size1 = as.numeric(abs(return1)) # size is indicator of volatility
direction1 = ifelse(return1 > 0, "up", ifelse(return1 < 0, "down", "same"))
date1 = as.Date(HO2$DATE[-1], "%m/%d/%Y")
price1 = as.numeric(diff(log(HO2$DHOILNYH)))
HO2.df1 = na.omit(data.frame(
    date = date1,
    price = price1,
    return = return1,
    size = size1,
    direction = direction1
))
```

The same dickey-fuller test was executed on the adjusted dataset:

```{r}
adf.test(HO2.df1$price)
```

Results indicate a `p-value=0.01`, which indicates the null hypothesis can be rejected. Specifically, the tranformed data is assumed to be stationary. Furthermore, autocorrelation seems very minor at `lag=4`, then a complete decay. Additionally, small partial-autocorrelation seem to exist with decaying characteristics.

```{r}
require(stats)
acf(HO2.df1$price)
pacf(HO2.df1$price)
```

Overall, the transformed data indicate appropriate properties for time-series analysis.

Some initial analysis on the `size`, and `return` indicate volatility in both measures. Specifically, early 1990's and 2000's exhibit significant volatility for `$DHOILNYH` oil price's, with some noticeable volatility around 1996. Corresponding `skewness=-1.4353`, and `kurtosis=38.2595` indicate a non-normal distribution. Based on earlier generated visualization, both the `size`, and `return` have the same pattern, with a general difference of scale. The `return` has much smaller values ranging from 0 to 5, while `size` has a range of roughly 15, with volatility values spiking between 40-50.

Another price measure, include the price direction between timesteps (n, n+1). In the case for `$DHOILNYH`, oil price decreasing, or going "down", occurred 3657 times. Similarly, oil price increasing, or going "up", occurred 3760 times, while no change (i.e. "same") occured 279 times.

To better summarize the above for all possible cases:

```{r}
table(HO2.df1$return < 0)
table(HO2.df1$return > 0)
table(HO2.df1$direction)
table(HO2.df1$return == 0)
```

The above summary seem equally dispersed, when accounting only for directional changes. Specifically, the difference between `TRUE`, or `FALSE` for each given case is not significantly different. However, this uniformity may be a result of the implemented transformation. Earlier generated pivot table help visualize the overall distribution in a more concise fashion:

```
direction	return.avg	return.sd	quantile.5	quantile.95	percent
down	-1.77	1.99	-4.78	-0.19	47.52
same	0.00	0.00	0.00	0.00	3.63
up	1.76	1.75	0.18	4.82	48.86
```

### Business Remarks
:
(REPLACE text here)
What can we say about the models we used for the data fit?
What can we do about the exposure that hits the variable cost of producing several products?