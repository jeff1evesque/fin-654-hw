---
title: 'Project #3: Escapades in Market Risk'
subtitle: 'Live Sessions: weeks 5 and 6'
output: flexdashboard::flex_dashboard
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, warning=FALSE, message=FALSE)
knitr::opts_chunk$set(tidy = TRUE)
knitr::opts_chunk$set(tidy.opts=list(width.cutoff=36))
knitr::opts_chunk$set(size = "small")
knitr::opts_hooks$set(fig.width = function(options) {
  if (options$fig.width < options$fig.height) {
    options$fig.width = options$fig.height
  }
  options
})
knitr::knit_hooks$set(mysize = function(before, options, envir) {
  if (before) 
    return(options$size)
})

options(repos=c(CRAN="http://archive.linux.duke.edu/cran/"))
install.packages(c('flexdashboard'))
```

```{r, include=FALSE}
library(ggplot2)
library(flexdashboard)
library(shiny)
library(QRM)
library(qrmdata)
library(xts)
library(zoo)
library(psych)
library(quantreg)
library(reshape2)

## analysis
rm(list = ls())

## PAGE: Exploratory Analysis
data = na.omit(read.csv("../data/metaldata.csv", header = TRUE))
prices = data

## Compute log differences percent using as.matrix to force numeric type
data.r = diff(log(as.matrix(data[, -1]))) * 100

##
## create size and direction
##
## Note: size is indicator of volatility
##
size = na.omit(abs(data.r))

colnames(size) = paste(colnames(size),".size", sep = "") # Teetor

## another indicator of volatility
direction = ifelse(data.r > 0, 1, ifelse(data.r < 0, -1, 0)) 
colnames(direction) = paste(colnames(direction),".dir", sep = "")

##
## convert into a time series object: 
##

## 1. split into date and rates
dates = as.Date(data$DATE[-1], "%m/%d/%Y")
dates.chr = as.character(data$DATE[-1])

values = cbind(data.r, size, direction)
data.df = data.frame(
  dates = dates,
  returns = data.r,
  size = size,
  direction = direction
)

data.df.nd = data.frame(
  dates = dates.chr,
  returns = data.r,
  size = size,
  direction = direction,
  stringsAsFactors = FALSE
) 

##
## non-coerced dates for subsetting on non-date columns
##

## 2. xts object with row names equal to the dates
data.xts = na.omit(as.xts(values, dates)) #order.by=as.Date(dates, "%d/%m/%Y")))
data.zr = as.zooreg(data.xts)
returns = data.xts

##
## PAGE: Market risk
##
corr_rolling = function(x) {	
  dim = ncol(x)	
  corr_r = cor(x)[lower.tri(diag(dim), diag = FALSE)]	
  return(corr_r)	
}
vol_rolling = function(x){
  library(matrixStats)
  vol_r = colSds(x)
  return(vol_r)
}

ALL.r = data.xts[, 1:3]
window = 90 #reactive({input$window})
corr_r = rollapply(
  ALL.r,
  width = window,
  corr_rolling,
  align = "right",
  by.column = FALSE
)
colnames(corr_r) = c("nickel.copper", "nickel.aluminium", "copper.aluminium")
vol_r = rollapply(
  ALL.r,
  width = window,
  vol_rolling,
  align = "right",
  by.column = FALSE
)
colnames(vol_r) = c("nickel.vol", "copper.vol", "aluminium.vol")
year = format(index(corr_r), "%Y")
r_corr_vol = merge(ALL.r, corr_r, vol_r, year)

##
## Market dependencies
##

#library(matrixStats)
R.corr = apply.monthly(as.xts(ALL.r), FUN = cor)
R.vols = apply.monthly(ALL.r, FUN = colSds) # from MatrixStats	

## Form correlation matrix for one month 	
R.corr.1 = matrix(R.corr[20,], nrow = 3, ncol = 3, byrow = FALSE)	
rownames(R.corr.1) = colnames(ALL.r[,1:3])	
colnames(R.corr.1) = rownames(R.corr.1)	
R.corr.1
R.corr = R.corr[, c(2, 3, 6)]
colnames(R.corr) = c("nickel.copper", "nickel.aluminium", "copper.aluminium") 	
colnames(R.vols) = c("nickel.vols", "copper.vols", "aluminium.vols")	
R.corr.vols = na.omit(merge(R.corr, R.vols))
year = format(index(R.corr.vols), "%Y")
R.corr.vols.y = data.frame(
  nickel.correlation = R.corr.vols[,1],
  copper.volatility = R.corr.vols[,5],
  year = year
)
nickel.vols = as.numeric(R.corr.vols[,"nickel.vols"])	
copper.vols = as.numeric(R.corr.vols[,"copper.vols"])	
aluminium.vols = as.numeric(R.corr.vols[,"aluminium.vols"])

## Roger Koenker UI Bob Hogg and Allen Craig
taus = seq(.05,.95,.05)
fit.rq.nickel.copper = rq(log(nickel.copper) ~ log(copper.vol), tau = taus, data = r_corr_vol)	
fit.lm.nickel.copper = lm(log(nickel.copper) ~ log(copper.vol), data = r_corr_vol)	
ni.cu.summary = summary(fit.rq.nickel.copper, se = "boot")

title.chg = "Metals Market Percent Changes"
```

```{r , echo=FALSE}
## pacf here
one = ts(data.df$returns.nickel)
two = ts(data.df$returns.copper)

## build function to repeat these routines
run_ccf = function(one, two, main = title.chg, lag = 20, color = "red"){
  # one and two are equal length series
  # main is title
  # lag is number of lags in cross-correlation
  # color is color of dashed confidence interval bounds
  stopifnot(length(one) == length(two))
  one = ts(one)
  two = ts(two)
  main = main
  lag = lag
  color = color
  ccf(one, two, main = main, lag.max = lag, xlab = "", ylab = "", ci.col = color)
}

##
## Load the data_moments() function
## data_moments function
## INPUTS: r vector
## OUTPUTS: list of scalars (mean, sd, median, skewness, kurtosis)
##
data_moments = function(data) {
  library(moments)
  library(matrixStats)
  mean.r = colMeans(data)
  median.r = colMedians(data)
  sd.r = colSds(data)
  IQR.r = colIQRs(data)
  skewness.r = skewness(data)
  kurtosis.r = kurtosis(data)
  result = data.frame(
    mean = mean.r,
    median = median.r,
    std_dev = sd.r,
    IQR = IQR.r,
    skewness = skewness.r,
    kurtosis = kurtosis.r
  )
  return(result)
}

## Run data_moments()
answer = data_moments(data.xts[, 5:8])

alpha = 0.95

## returns 1
returns1 = returns[,1]
colnames(returns1) = "Returns" #kluge to coerce column name for df
returns1.df = data.frame(
    Returns = returns1[,1],
    Distribution = rep("Historical", each = length(returns1))
)
  
## Value at Risk
VaR1.hist = quantile(returns1,alpha)
VaR1.text = paste("Value at Risk =", round(VaR1.hist, 2))

## Determine the max y value of the desity plot.
## This will be used to place the text above the plot
VaR1.y = max(density(returns1.df$Returns)$y)

# Expected Shortfall
ES1.hist = mean(returns1[returns1 > VaR1.hist])
ES1.text = paste("Expected Shortfall =", round(ES1.hist, 2))

## returns 2
returns2 = returns[,2]
colnames(returns2) = "Returns" #kluge to coerce column name for df
returns2.df = data.frame(
    Returns = returns2[,1],
    Distribution = rep("Historical", each = length(returns2))
)

## Value at Risk
VaR2.hist = quantile(returns2,alpha)
VaR2.text = paste("Value at Risk =", round(VaR2.hist, 2))

## Determine the max y value of the desity plot.
## This will be used to place the text above the plot
VaR2.y = max(density(returns2.df$Returns)$y)

## Expected Shortfall
ES2.hist = mean(returns1[returns2 > VaR2.hist])
ES2.text = paste("Expected Shortfall =", round(ES2.hist, 2))

## returns 3
returns3 = returns[,3]
colnames(returns3) = "Returns" #kluge to coerce column name for df
returns3.df = data.frame(
    Returns = returns3[,1],
    Distribution = rep("Historical", each = length(returns3))
)
  
## Value at Risk
VaR3.hist = quantile(returns3,alpha)
VaR3.text = paste("Value at Risk =", round(VaR3.hist, 2))

## Determine the max y value of the desity plot.
## This will be used to place the text above the plot
VaR3.y = max(density(returns3.df$Returns)$y)

# Expected Shortfall
ES3.hist = mean(returns1[returns3 > VaR3.hist])
ES3.text = paste("Expected Shortfall =", round(ES3.hist, 2))
```

# Metal

## Column {data-width=600 .tabset}

### Sizes

#### CCF

```{r, echo=FALSE}
one = ts(data.zr[,2])
two = ts(data.zr[,3])
title = "Copper vs. Aluminum"
run_ccf(one, two, main = title, lag = 20, color = "red")
```

#### Volatility

```{r, echo=FALSE}
one = abs(data.zr[,2])
two = abs(data.zr[,3])
title = "Copper vs. Aluminum"
run_ccf(one, two, main = title, lag = 20, color = "red")
```

#### CCF

```{r, echo=FALSE}
one = ts(data.zr[,1])
two = ts(data.zr[,2])
title = "Nickel vs. Copper"
run_ccf(one, two, main = title, lag = 20, color = "red")
```

#### Volatility

```{r, echo=FALSE}
one = abs(data.zr[,1])
two = abs(data.zr[,2])
title = "Nickel vs. Copper"
run_ccf(one, two, main = title, lag = 20, color = "red")
```

#### CCF

```{r, echo=FALSE}
one = ts(data.zr[,1])
two = ts(data.zr[,3])
title = "Nickel vs. Aluminum"
run_ccf(one, two, main = title, lag = 20, color = "red")
```

#### Volatility

```{r, echo=FALSE}
one = abs(data.zr[,1])
two = abs(data.zr[,3])
title = "Nickel vs. Aluminum"
run_ccf(one, two, main = title, lag = 20, color = "red")
```

### Moments

```{r, echo=FALSE}
answer = round(answer, 4)
knitr::kable(answer)
```

## Column {data-width=600 .tabset}

### Nickel

```{r, echo=FALSE}
ggplot(
    returns1.df,
    aes(x = Returns, fill = Distribution)) +
    geom_density(alpha = 0.5) +
    geom_vline(aes(xintercept = VaR1.hist), linetype = "dashed", size = 1, color = "firebrick1") +
    geom_vline(aes(xintercept = ES1.hist), size = 1, color = "firebrick1") +
    annotate("text", x = 2+ VaR1.hist, y = VaR1.y*1.05, label = VaR1.text) +
    annotate("text", x = 1.5+ ES1.hist, y = VaR1.y*1.1, label = ES1.text) +
    scale_fill_manual( values = "dodgerblue4") +
    ggtitle('Nickel Returns')
```

### Copper

```{r, echo=FALSE}
ggplot(
    returns2.df,
    aes(x = Returns, fill = Distribution)) +
    geom_density(alpha = 0.5) +
    geom_vline(aes(xintercept = VaR2.hist), linetype = "dashed", size = 1, color = "firebrick1") +
    geom_vline(aes(xintercept = ES2.hist), size = 1, color = "firebrick1") +
    annotate("text", x = 2+ VaR2.hist, y = VaR1.y*1.05, label = VaR2.text) +
    annotate("text", x = 1.5+ ES2.hist, y = VaR1.y*1.1, label = ES2.text) +
    scale_fill_manual( values = "dodgerblue4") +
    ggtitle('Copper Returns')
```

### Aluminum

```{r, echo=FALSE}
ggplot(
    returns3.df,
    aes(x = Returns, fill = Distribution)) +
    geom_density(alpha = 0.5) +
    geom_vline(aes(xintercept = VaR3.hist), linetype = "dashed", size = 1, color = "firebrick1") +
    geom_vline(aes(xintercept = ES3.hist), size = 1, color = "firebrick1") +
    annotate("text", x = 2+ VaR3.hist, y = VaR1.y*1.05, label = VaR3.text) +
    annotate("text", x = 1.5+ ES3.hist, y = VaR3.y*1.1, label = ES3.text) +
    scale_fill_manual( values = "dodgerblue4") +
    ggtitle('Aluminum Returns')
```

# Returns & Sizes

## Column {data-width=600 .tabset}

### ACF: Returns

```{r, echo=FALSE}
acf(coredata(data.xts[,1:3]))
```

### ACF: Sizes

```{r, echo=FALSE}
acf(coredata(data.xts[,4:6])) # sizes
```

## Column {data-width=600 .tabset}

### Returns

```{r, echo=FALSE}
autoplot.zoo(data.xts[,1:3]) + ggtitle(title.chg) + ylim(-5, 5)
```

### Sizes

```{r, echo=FALSE}
autoplot.zoo(data.xts[,4:6]) + ggtitle(title.chg) + ylim(-5, 5)
```

```{r, echo=FALSE}
##
## Now for Loss Analysis
##

price.last = as.numeric(tail(data[, -1], n=1))

## Specify the positions
position.rf = c(1/3, 1/3, 1/3)

## And compute the position weights
w = position.rf * price.last

## Fan these  the length and breadth of the risk factor series
weights.rf = matrix(w, nrow=nrow(data.r), ncol=ncol(data.r), byrow=TRUE)
#head(rowSums((exp(data.r/100)-1)*weights.rf), n=3)

## exp, we will either earn or lose from original dollar
## We need to compute exp(x) - 1 for very small x: expm1 accomplishes this
## head(rowSums((exp(data.r/100)-1)*weights.rf), n=4
## weights.rf, is todays values
## expm1(data.r/100) == (e^x) - 1 (m1 means -1, euler)
## data.r/100 == x
## data.r, is the return, which is normalized to compare against the dollar
## e^x is the value tomorrow, taking the log brings us back to x

loss.rf = -rowSums(expm1(data.r/100) * weights.rf)
loss.rf.df = data.frame(Loss = loss.rf, Distribution = rep("Historical", each = length(loss.rf)))

## trying to find good threshold
##
## exceedance line slices the curve

## Simple Value at Risk and Expected Shortfall
alpha.tolerance = .95
VaR.hist = quantile(loss.rf, probs=alpha.tolerance, names=FALSE)

## Just as simple Expected shortfall
ES.hist = mean(loss.rf[loss.rf > VaR.hist])
VaR.text = paste("Value at Risk =\n", round(VaR.hist, 2)) # ="VaR"&c12
ES.text = paste("Expected Shortfall \n=", round(ES.hist, 2))
title.text = paste(round(alpha.tolerance*100, 0), "% Loss Limits")

# mean excess plot to determine thresholds for extreme event management
data = as.vector(loss.rf) # data is purely numeric
umin =  min(data)         # threshold u min
umax =  max(data) - 0.1   # threshold u max
nint = 100                # grid length to generate mean excess plot
grid.0 = numeric(nint)    # grid store
e = grid.0                # store mean exceedances e
upper = grid.0            # store upper confidence interval
lower = grid.0            # store lower confidence interval
u = seq(umin, umax, length = nint) # threshold u grid
alpha = 0.95                  # confidence level

for (i in 1:nint) {
  data = data[data > u[i]]  # subset data above thresholds
  e[i] = mean(data - u[i])  # calculate mean excess of threshold
  sdev = sqrt(var(data))    # standard deviation
  n = length(data)          # sample size of subsetted data above thresholds
  upper[i] = e[i] + (qnorm((1 + alpha)/2) * sdev)/sqrt(n) # upper confidence interval
  lower[i] = e[i] - (qnorm((1 + alpha)/2) * sdev)/sqrt(n) # lower confidence interval
}

mep.df = data.frame(threshold = u, threshold.exceedances = e, lower = lower, upper = upper)
loss.excess = loss.rf[loss.rf > u]

##
# GPD to describe and analyze the extremes
#
#library(QRM)
alpha.tolerance = 0.95
u = quantile(loss.rf, alpha.tolerance , names=FALSE)
fit = fit.GPD(loss.rf, threshold=u) # Fit GPD to the excesses
xi.hat = fit$par.ests[["xi"]] # fitted xi
beta.hat = fit$par.ests[["beta"]] # fitted beta
data = loss.rf
n.relative.excess = length(loss.excess) / length(loss.rf) # = N_u/n
VaR.gpd = u + (beta.hat/xi.hat)*(((1-alpha.tolerance) / n.relative.excess)^(-xi.hat)-1)
ES.gpd = (VaR.gpd + beta.hat-xi.hat*u) / (1-xi.hat)

# Plot away
VaRgpd.text = paste("GPD: Value at Risk =", round(VaR.gpd, 2))
ESgpd.text = paste("Expected Shortfall =", round(ES.gpd, 2))
title.text = paste(VaRgpd.text, ESgpd.text, sep = " ")
```

# Loss Analysis

## Column {data-width=600 .tabset}

### Count

```{r, echo=FALSE}
title.text = paste0(round(alpha.tolerance * 100, 0), "% Loss Limits")
ggplot(loss.rf.df, aes(x = Loss, fill = Distribution)) +
  geom_histogram(alpha = 0.8) +
  geom_vline(aes(xintercept = VaR.hist), linetype = "dashed", size = 1, color = "blue") +
  geom_vline(aes(xintercept = ES.hist), size = 1, color = "blue") +
  annotate("text", x = VaR.hist, y = 40, label = VaR.text) +
  annotate("text", x = ES.hist, y = 20, label = ES.text) +
    xlim(0, 500) +
  ggtitle(title.text)
```

### Density

```{r, echo=FALSE}
title.text = paste0('GPD: ', round(alpha.tolerance * 100, 0), "% Loss Limits")
ggplot(loss.rf.df, aes(x = Loss, fill = Distribution)) +
  geom_density(alpha = 0.2) +
  geom_vline(aes(xintercept = VaR.gpd), colour = "blue", linetype = "dashed", size = 0.8) +
  geom_vline(aes(xintercept = ES.gpd), colour = "blue", size = 0.8) +
  annotate("text", x = 300, y = 0.005, label = ESgpd.text, colour = "blue") +
  xlim(0,500) +
  ggtitle(title.text)
```

## Column {data-width=600 .tabset}

### Value at Risk

```{r, echo=FALSE}
showRM(fit, alpha = 0.99, RM = "VaR", method = "BFGS")
```

### Expected Shortfall

```{r, echo=FALSE}
showRM(fit, alpha = 0.99, RM = "ES", method = "BFGS")
```

# Final Remarks

## Column {data-width=600 .tabset}

### Methods

```
The following R utilities were used and provided facilities to build custom functions required for exploration and analysis in this study:

- flexdashboard, creates HTML with row and colum formatting
- rq, quantile regression
- as.zooreg, creates series-like object
- autoplot.zoo, plotting for time-series
- acf, autocorrelation
- expm1, computes the exponential of the given value minus one
- showRM, plot estimated tail probabilities
- ccf, measure similarity between two series
```

### Data Insights

```
The strongest cross-correlation sequentially:

- Nickel vs Copper: 7
- Nickel vs Aluminum: 5
- Copper vs Aluminum: 3

Note: above numbers are suggestive lags for achieving stationarity.

The corresponding timeseries suggests most fluctuations with nickel, followed by copper (via `Size` timeseries), then aluminum. Since the value at risk (VaR) is the largest likely loss for a given portfolio, and nickel has greater expected shortfall (ES), this indicates a considerably risky investment.

The overall loss limit exhibits an exponential tail. Without further proof whether the overall distribution is normal, suggests the use of ES over VaR. Furthermore, the general pareto distribution (GPD) provides better accuracy by relying less on the distribution central mode. Since calculations focuses directly on the exceedance, results will better fit the data subset.

Using GPD, the expected shortfall computed at 450.9, more than double the original 194.46. This significant difference could be detrimental for an investment should the risk occur. Lastly, the "Estimated tail probabilities" indicates the GPD having greater variance for ES compared to VaR. As a follow-up, it would be interesting to determine whether the VaR is normal, and therefore applicable.
```

### Business Remarks

```
1. How would the performance of these commodities affect the size and timing of shipping arrangements?

Performance can be measured by defining a Value at Risk (VaR), which defines the maximum expected loss with a given confidence. An associated expected shortfall (ES), is the average of all loses that exceed the VaR. Using previous ES against an accepted risk level, will ensure the right amount of commodities are produced. Specifically, an abundance of non-distributed commodities may overwhelm transportation means. Likewise, not maximizing the commodities for a given shipping location, corresponds to under-utilizing shipping resources. Therefore, VaR and ES would likely be incorporated as a series 'what if' scenarios, with varying commodities at different shipping location. Performance can be measured by an overall net sale, or net sale relative to shipped goods.

2. How would the value of new shipping arrangements affect the value of our business with our current customers?

New shipping arrangements could pivot away companies from current customers to tramp trade, if shipment is more lucrative. This would hurt the current supply chain relationships.

3. How would we manage the allocation of existing resources given we have just landed in this new market?

Understanding the price sensitivity of the metals market (using the provided graphs), is an important requirement for accurate asset allotment. Furthermore, information such as VAR and ES, allows resources and budget to be properly distributed, with the intention to improves processes, without negative side-effects to existing business model(s). 
```

### Definitions

```
Risk Measure: four properties are generally required by a risk measure

- monotonicity: if the investment portfolio generates higher returns than another investment, throughout the world, the investment has less risk.
- translation invariance: if more money is added to the portfolio, the risk is reduced by that amount.
- homogeneity: if the portfolio is increased by a factor, then the risk will increase by the same factor.
- subadditivity: the sum of two combined portfolio will be less than the sum of the risk individually.

It is important to note, all four cases are satisfied by ES. However, the fourth case fails for VaR when the distribution is non-normal.

Source:

- https://quantdare.com/value-at-risk-or-expected-shortfall/
- https://www.mathworks.com/help/stats/generalized-pareto-distribution.html
- https://github.com/jeff1evesque/fin-654-hw/blob/master/docs/me20-1-4.pdf
```

## Column {data-width=600}

### threshold

```{r, echo=FALSE}
ggplot(mep.df, aes( x= threshold, y = threshold.exceedances)) + geom_line() + geom_line(aes(x = threshold, y = lower), colour = "red") + geom_line(aes(x = threshold,  y = upper), colour = "red") + annotate("text", x = 400, y = 200, label = "upper 95%") + annotate("text", x = 200, y = 0, label = "lower 5%")
```